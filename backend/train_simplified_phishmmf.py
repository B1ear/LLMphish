"""
ä½¿ç”¨å¯æå–ç‰¹å¾é‡æ–°è®­ç»ƒ PhishMMF æ¨¡å‹ã€‚

åªä½¿ç”¨æˆ‘ä»¬å¯ä»¥ä»é‚®ä»¶ä¸­ç›´æ¥æå–çš„ 35 ä¸ªç‰¹å¾ï¼š
- æ–‡æœ¬ç‰¹å¾ï¼šä¸»é¢˜ (6) + å‘ä»¶äºº (2) + æ­£æ–‡ (16)
- URL åŸºç¡€ç‰¹å¾ (11)

æ€»è®¡ï¼š35 ç»´ç‰¹å¾
"""

import json
import numpy as np
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib


def extract_simplified_features(data: dict) -> list:
    """
    ä» JSON æ•°æ®ä¸­æå–å¯è·å–çš„ 35 ç»´ç‰¹å¾
    """
    features = []
    
    # 1. æ–‡æœ¬ç‰¹å¾ - ä¸»é¢˜ (6ç»´)
    subject = data.get("text_features", {}).get("subject", {})
    urgency_map = {"Low": 0, "Moderate": 1, "High": 2, "unknown": 0, "Ne": 0}
    features.append(urgency_map.get(subject.get("urgency_level", "unknown"), 0))
    features.append(int(subject.get("contains_threatening_language", False)))
    features.append(int(subject.get("contains_seductive_language", False)))
    features.append(int(subject.get("contains_emergency_action_request", False)))
    features.append(float(subject.get("sentiment_score", 0.0)))
    sentiment_map = {"Negative": 0, "Neutral": 1, "Positive": 2}
    features.append(sentiment_map.get(subject.get("sentiment_label", "Neutral"), 1))
    
    # 2. æ–‡æœ¬ç‰¹å¾ - å‘ä»¶äºº (2ç»´)
    sender = data.get("text_features", {}).get("sender", {})
    impersonation_map = {
        "unknown": 0, "None": 0, "Bank": 1, "Government": 2, 
        "E-commerce": 3, "Social Media": 4
    }
    features.append(impersonation_map.get(sender.get("impersonation_type", "unknown"), 0))
    anomaly_map = {
        "unknown": 0, "None": 0, "Non-official domain": 1, 
        "Spelling error": 2, "Suspicious": 1
    }
    features.append(anomaly_map.get(sender.get("email_address_anomalies", "unknown"), 0))
    
    # 3. æ–‡æœ¬ç‰¹å¾ - æ­£æ–‡ (16ç»´)
    content = data.get("text_features", {}).get("content", {})
    features.append(int(content.get("word_count", 0)))
    features.append(int(content.get("url_count", 0)))
    features.append(int(content.get("spelling_errors", 0)))
    features.append(int(content.get("grammar_errors", 0)))
    features.append(len(content.get("suspicious_keywords", [])))
    features.append(int(content.get("urgency_words_count", 0)))
    features.append(int(content.get("contains_personal_information_request", False)))
    features.append(int(content.get("contains_abnormal_financial_request", False)))
    features.append(float(content.get("text_complexity", 0.0)))
    features.append(float(content.get("text_similarity_to_legitimate_emails", 0.0)))
    lang_map = {"en": 0, "zh": 1, "mixed": 2, "unknown": 0}
    features.append(lang_map.get(content.get("language", "en"), 0))
    features.append(int(content.get("contains_obfuscated_text", False)))
    features.append(int(content.get("requests_otp_or_mfa", False)))
    features.append(int(content.get("contains_phishing_call_to_action", False)))
    sentiment_map2 = {"Negative": 0, "Neutral": 1, "Positive": 2}
    features.append(sentiment_map2.get(content.get("text_sentiment", "Neutral"), 1))
    features.append(float(content.get("text_sentiment_score", 0.0)))
    
    # 4. URL åŸºç¡€ç‰¹å¾ (11ç»´)
    url_basic = data.get("url_intelligence_features", {}).get("basic", {})
    features.append(int(url_basic.get("domain_length", 0)))
    features.append(int(url_basic.get("dot_count", 0)))
    features.append(int(url_basic.get("contains_ip_address", False)))
    features.append(int(url_basic.get("contains_at_symbol", False)))
    features.append(int(url_basic.get("contains_hyphen", False)))
    features.append(int(url_basic.get("path_length", 0)))
    features.append(int(url_basic.get("subdomains_count", 0)))
    tld_map = {"com": 0, "org": 1, "net": 2, "edu": 3, "gov": 4, "other": 5, "unknown": 5}
    tld = url_basic.get("tld", "unknown")
    if tld not in tld_map:
        tld = "other"
    features.append(tld_map.get(tld, 5))
    features.append(int(url_basic.get("query_params_count", 0)))
    features.append(int(url_basic.get("has_suspicious_query_params", False)))
    features.append(len(url_basic.get("suspicious_query_params", [])))
    
    return features


def load_data():
    """åŠ è½½ PhishMMF æ•°æ®"""
    jsonl_path = Path("PhishMMF-main/all/all.jsonl")
    labels_path = Path("PhishMMF-main/email_phishing_labels.npy")
    
    if not jsonl_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        return None, None
    
    if not labels_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {labels_path}")
        return None, None
    
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    
    # åŠ è½½æ ‡ç­¾
    labels = np.load(labels_path)
    print(f"  æ ‡ç­¾æ•°é‡: {len(labels)}")
    print(f"  é’“é±¼é‚®ä»¶: {np.sum(labels == 1)} ({np.sum(labels == 1)/len(labels)*100:.1f}%)")
    print(f"  æ­£å¸¸é‚®ä»¶: {np.sum(labels == 0)} ({np.sum(labels == 0)/len(labels)*100:.1f}%)")
    
    # åŠ è½½ç‰¹å¾
    features_list = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i % 1000 == 0:
                print(f"  å¤„ç†ä¸­: {i}/{len(labels)}", end='\r')
            
            line = line.strip()
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            
            try:
                data = json.loads(line)
                features = extract_simplified_features(data)
                features_list.append(features)
            except json.JSONDecodeError as e:
                print(f"\nâš ï¸  JSONè§£æé”™è¯¯ (è¡Œ {i}): {e}")
                continue
    
    print(f"  å¤„ç†å®Œæˆ: {len(features_list)}/{len(labels)}")
    
    X = np.array(features_list, dtype=float)
    y = labels
    
    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  ç‰¹å¾çŸ©é˜µ: {X.shape}")
    print(f"  æ ‡ç­¾å‘é‡: {y.shape}")
    print(f"  ç‰¹å¾èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
    print(f"  ç‰¹å¾å‡å€¼: {X.mean():.4f}")
    print(f"  ç‰¹å¾æ ‡å‡†å·®: {X.std():.4f}")
    
    return X, y


def train_models(X, y):
    """è®­ç»ƒç®€åŒ–æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ¯ è®­ç»ƒç®€åŒ– PhishMMF æ¨¡å‹")
    print("="*70)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nğŸ“Š æ•°æ®åˆ’åˆ†:")
    print(f"  è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    print(f"\nğŸ”§ ç‰¹å¾æ ‡å‡†åŒ–...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"  æ ‡å‡†åŒ–åèŒƒå›´: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]")
    print(f"  æ ‡å‡†åŒ–åå‡å€¼: {X_train_scaled.mean():.4f}")
    print(f"  æ ‡å‡†åŒ–åæ ‡å‡†å·®: {X_train_scaled.std():.4f}")
    
    # è®­ç»ƒ RandomForest
    print(f"\nğŸŒ² è®­ç»ƒ RandomForest...")
    rf = RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    rf.fit(X_train_scaled, y_train)
    
    # è¯„ä¼° RandomForest
    print(f"\nğŸ“Š RandomForest è¯„ä¼°:")
    y_pred_rf = rf.predict(X_test_scaled)
    y_proba_rf = rf.predict_proba(X_test_scaled)[:, 1]
    
    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred_rf, target_names=["æ­£å¸¸", "é’“é±¼"]))
    
    print(f"æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_test, y_pred_rf)
    print(f"  TN={cm[0,0]}, FP={cm[0,1]}")
    print(f"  FN={cm[1,0]}, TP={cm[1,1]}")
    
    auc_rf = roc_auc_score(y_test, y_proba_rf)
    print(f"AUC-ROC: {auc_rf:.4f}")
    
    # äº¤å‰éªŒè¯
    cv_scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
    print(f"äº¤å‰éªŒè¯ AUC: {cv_scores_rf.mean():.4f} Â± {cv_scores_rf.std():.4f}")
    
    # è®­ç»ƒ XGBoost
    print(f"\nğŸš€ è®­ç»ƒ XGBoost...")
    xgb = XGBClassifier(
        n_estimators=100,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbosity=1
    )
    xgb.fit(X_train_scaled, y_train)
    
    # è¯„ä¼° XGBoost
    print(f"\nğŸ“Š XGBoost è¯„ä¼°:")
    y_pred_xgb = xgb.predict(X_test_scaled)
    y_proba_xgb = xgb.predict_proba(X_test_scaled)[:, 1]
    
    print(f"\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred_xgb, target_names=["æ­£å¸¸", "é’“é±¼"]))
    
    print(f"æ··æ·†çŸ©é˜µ:")
    cm = confusion_matrix(y_test, y_pred_xgb)
    print(f"  TN={cm[0,0]}, FP={cm[0,1]}")
    print(f"  FN={cm[1,0]}, TP={cm[1,1]}")
    
    auc_xgb = roc_auc_score(y_test, y_proba_xgb)
    print(f"AUC-ROC: {auc_xgb:.4f}")
    
    # äº¤å‰éªŒè¯
    cv_scores_xgb = cross_val_score(xgb, X_train_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1)
    print(f"äº¤å‰éªŒè¯ AUC: {cv_scores_xgb.mean():.4f} Â± {cv_scores_xgb.std():.4f}")
    
    # ç‰¹å¾é‡è¦æ€§
    print(f"\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§ (Top 10):")
    feature_names = [
        "urgency_level", "threatening", "seductive", "emergency", "sentiment_score", "sentiment_label",
        "impersonation", "email_anomaly",
        "word_count", "url_count", "spelling_errors", "grammar_errors", "suspicious_keywords",
        "urgency_words", "personal_info_request", "financial_request", "text_complexity",
        "similarity_to_legit", "language", "obfuscated", "otp_request", "phishing_cta",
        "text_sentiment", "text_sentiment_score",
        "domain_length", "dot_count", "ip_address", "at_symbol", "hyphen",
        "path_length", "subdomains", "tld", "query_params", "suspicious_params", "suspicious_params_list"
    ]
    
    importances = rf.feature_importances_
    indices = np.argsort(importances)[::-1][:10]
    
    for i, idx in enumerate(indices, 1):
        print(f"  {i:2d}. {feature_names[idx]:25s}: {importances[idx]:.4f}")
    
    return rf, xgb, scaler


def save_models(rf, xgb, scaler):
    """ä¿å­˜æ¨¡å‹"""
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    
    models_dir = Path("backend/models")
    models_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    joblib.dump(rf, models_dir / "phishmmf_simplified_rf.joblib")
    joblib.dump(xgb, models_dir / "phishmmf_simplified_xgb.joblib")
    joblib.dump(scaler, models_dir / "phishmmf_simplified_scaler.joblib")
    
    print(f"  âœ… RandomForest: {models_dir / 'phishmmf_simplified_rf.joblib'}")
    print(f"  âœ… XGBoost: {models_dir / 'phishmmf_simplified_xgb.joblib'}")
    print(f"  âœ… Scaler: {models_dir / 'phishmmf_simplified_scaler.joblib'}")


def main():
    print("ğŸ” ç®€åŒ– PhishMMF æ¨¡å‹è®­ç»ƒ")
    print("="*70)
    print("ä½¿ç”¨ 35 ä¸ªå¯æå–ç‰¹å¾:")
    print("  - æ–‡æœ¬ç‰¹å¾: 24 ç»´ (ä¸»é¢˜ + å‘ä»¶äºº + æ­£æ–‡)")
    print("  - URL åŸºç¡€ç‰¹å¾: 11 ç»´")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    X, y = load_data()
    if X is None or y is None:
        return
    
    # è®­ç»ƒæ¨¡å‹
    rf, xgb, scaler = train_models(X, y)
    
    # ä¿å­˜æ¨¡å‹
    save_models(rf, xgb, scaler)
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print("\nä¸‹ä¸€æ­¥:")
    print("1. å®ç°å¯¹åº”çš„ç‰¹å¾æå–ä»£ç  (backend/app/simplified_phishmmf_features.py)")
    print("2. æ›´æ–°æ¨¡å‹åŠ è½½ä»£ç  (backend/app/model.py)")
    print("3. æµ‹è¯•ç®€åŒ–æ¨¡å‹æ•ˆæœ")


if __name__ == "__main__":
    main()
